# machinelearninginaction





**标称型**：标称型目标变量的结果只在有限目标集中取值，如真与假(标称型目标变量主要用于分类) 也就相当于枚举类型

**数值型**：数值型目标变量则可以从无限的数值集合中取值，如0.100，42.001等 (数值型目标变量主要用于回归分析)



# ch2 KNN

定义：根据最邻近的k个点的分类来决定分类结果

优点：对异常值不敏感，无数据输入假定，可以多分类

缺点： 计算复杂度高，空间复杂度高

适用范围：数值型，标称型

## 算法流程：

1.收集数据，准备数据，分析数据

2.测试算法  调整参数k

3.使用算法

没有训练步骤

## 分类核心过程：

1. 计算待测试点与数据集中点的距离
2. 排序选取距离最小的k个点
3. 选择k个点中出现频率最高的类别作为预测结果的分类

## 调整内容：

1. 参数K

2. 距离计算方式

   都会影响算法的结果，具体问题具体分析

[demo:](<https://github.com/sun8904/machinelearninginaction/tree/master/Ch02>) 



# ch3 决策树

定义 构造一颗可以分类的树

优点：计算复杂度不高，结果易理解，可以处理不相关的特征，对中间值缺失不敏感，可以多分类

缺点：过度匹配

## 算法流程：

1. 收集，准备，分析数据，数值型必须离散化

2. 训练：构造树
3. 测试: 测试集利用训练树来分类测试
4. 使用 不依赖训练集，只需要训练出的树结构。

## 核心：

信息增益：表示含信息量多少

每个特征按照信息量从大到小排序，按照特征来分类构造树，某个节点的准确率达到一定的阈值，就作为一个叶子节点。

## 调整内容：

满足一个叶子节点的条件，阈值越高，分类越准确，树也更复杂

对训练集也比较敏感



# ch4 朴素贝叶斯

利用先验概率（统计）来求后验概率（条件概率）的问题。

优点：数据少也有效（数据能反映整体情况下是个前提），可以多分类（可以算出每个类别的概率，当然可以支持多分类）

缺点：数据敏感（影响先验，就会影响后验，最终也会影响分类）

## 算法流程：

1. 收集，准备，分析
2. 训练：计算不同特征的条件概率
3. 测试：错误率
4. 使用：利用训练集计算的概率来验证，不依赖数据集，增量更新数据集就会影响概率了。

核心：

最常见的是文本分类，统计学来说，一个特征需要N个样本，

10个特征就是N的10次方，假设特征是独立的，就变成N*10个数据集了。

